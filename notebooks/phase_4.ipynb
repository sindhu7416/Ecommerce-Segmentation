{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7bb0004",
   "metadata": {},
   "source": [
    "# Phase 4: Predictive Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89517ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve\n",
    "import shap\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b579fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clustered data from Phase 3\n",
    "df = pd.read_csv('clustered_customers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b763aa6",
   "metadata": {},
   "source": [
    "## SHAP Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65f2cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate SHAP values\n",
    "explainer = shap.TreeExplainer(best_rf)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Handle binary classification SHAP values\n",
    "if isinstance(shap_values, list) and len(shap_values) == 2:\n",
    "    # Binary classification case\n",
    "    shap_values = shap_values[1]  # We want values for class 1 (positive response)\n",
    "elif len(shap_values.shape) == 3:\n",
    "    # Multi-class format, select class 1\n",
    "    shap_values = shap_values[:, :, 1]\n",
    "\n",
    "# Verify shapes match\n",
    "assert shap_values.shape == X_test.shape, \\\n",
    "    f\"SHAP values shape {shap_values.shape} doesn't match X_test shape {X_test.shape}\"\n",
    "\n",
    "#  Global Feature Importance (Matplotlib)\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\", show=False)\n",
    "plt.title(\"Top Features Driving Campaign Response\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Individual Prediction Explanation (HTML)\n",
    "sample_idx = 0  # First test case\n",
    "shap.initjs()  # Initialize JS visualization\n",
    "force_plot = shap.force_plot(\n",
    "    explainer.expected_value[1],\n",
    "    shap_values[sample_idx, :],\n",
    "    X_test.iloc[sample_idx, :],\n",
    "    feature_names=X_test.columns.tolist(),\n",
    "    matplotlib=False\n",
    ")\n",
    "\n",
    "# Save interactive plot\n",
    "shap.save_html('shap_force_plot.html', force_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a003f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: Beeswarm Plot for Detailed Analysis\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values, X_test, show=False)\n",
    "plt.title(\"Feature Impact on Campaign Response\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_beeswarm.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"SHAP outputs saved successfully:\")\n",
    "print(\"- Global feature importance: shap_feature_importance.png\")\n",
    "print(\"- Detailed impact analysis: shap_beeswarm.png\")\n",
    "print(\"- Interactive explanation: shap_force_plot.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a5c2de",
   "metadata": {},
   "source": [
    "## Response Probability by Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b2284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Response_Probability'] = best_rf.predict_proba(X)[:, 1]\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(\n",
    "    x='Cluster', \n",
    "    y='Response_Probability', \n",
    "    data=df,\n",
    "    order=sorted(df['Cluster'].unique()),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Response Probability by Customer Segment', fontsize=14)\n",
    "plt.xlabel('Segment')\n",
    "plt.ylabel('Response Probability')\n",
    "plt.xticks(ticks=range(len(df['Cluster'].unique())),\n",
    "           labels=[f'Segment {i+1}' for i in sorted(df['Cluster'].unique())])\n",
    "plt.savefig('response_by_segment.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26408533",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0befa726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save probabilities with cluster info\n",
    "output_cols = ['ID', 'Cluster'] + features + ['Response_Probability']\n",
    "df[output_cols].to_csv('customers_with_predictions.csv', index=False)\n",
    "\n",
    "# Generate segment-level insights\n",
    "segment_stats = df.groupby('Cluster')['Response_Probability'].agg(\n",
    "    ['mean', 'median', 'std', 'count']\n",
    ").reset_index()\n",
    "segment_stats.columns = ['Segment', 'Avg_Prob', 'Median_Prob', 'Std_Prob', 'Count']\n",
    "segment_stats.to_csv('segment_response_stats.csv', index=False)\n",
    "\n",
    "print(\"\\nOutput Files Created:\")\n",
    "print(\"- rf_model_with_segments.pkl: Trained model\")\n",
    "print(\"- shap_*.png: SHAP interpretation plots\")\n",
    "print(\"- response_by_segment.png: Segment comparison\")\n",
    "print(\"- customers_with_predictions.csv: Full dataset with predictions\")\n",
    "print(\"- segment_response_stats.csv: Summary statistics by segment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs685hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
